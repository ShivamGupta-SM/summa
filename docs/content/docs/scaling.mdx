---
title: Scaling
description: Scale Summa from thousands to millions of transactions with connection pooling, read replicas, Redis caching, event store partitioning, Redis Streams, CQRS projections, and hash snapshots.
icon: ChartBar
---

## Overview

Summa is designed to scale from a single-instance deployment to a distributed multi-instance cluster. This guide covers the built-in scalability features and when to enable them.

## Architecture at Scale

A production Summa deployment with all scalability features enabled:

```
                    ┌─────────────┐
                    │  Load       │
                    │  Balancer   │
                    └──────┬──────┘
               ┌───────────┼───────────┐
               ▼           ▼           ▼
         ┌──────────┐┌──────────┐┌──────────┐
         │ Summa    ││ Summa    ││ Summa    │
         │ Instance ││ Instance ││ Instance │
         └─────┬────┘└─────┬────┘└─────┬────┘
               │           │           │
               ▼           ▼           ▼
         ┌─────────────────────────────────┐
         │         Redis + Streams         │
         │  (rate limits, idempotency,     │
         │   message queue, projections)   │
         └─────────────────────────────────┘
               │           │
     ┌─────────┘           └──────┐
     ▼                            ▼
┌──────────┐               ┌──────────┐
│ Primary  │──── stream ──▶│ Replica  │
│   DB     │   replication │   DB     │
│ (writes) │               │ (reads + │
│          │               │ projections)
└──────────┘               └──────────┘
  partitioned                read models
  ledger_event               (CQRS)
```

## Estimated Throughput

| Configuration | Estimated TPS | Notes |
|---------------|:---:|-------|
| Single instance, default config | 1,000–5,000 | Limited by per-txn round-trips and single DB |
| + Connection pooling | 3,000–8,000 | Better connection reuse |
| + Denormalized balance + lock retry | 5,000–15,000 | Fewer queries per txn, fail-fast lock contention |
| + Mega CTE (combined writes) | 8,000–20,000 | All writes in a single SQL round-trip |
| + Read replicas | 15,000–30,000 | Offloads read queries |
| + Redis secondary storage | 20,000–40,000 | Distributed rate limiting, multi-instance |
| + Hot accounts + version retention | 30,000–60,000 | Reduces contention + keeps tables bounded |
| + Transaction batching (batch engine) | 60,000–120,000+ | Amortizes overhead across N transactions |
| + Event store partitioning | — | Keeps query performance constant as data grows |
| + Redis Streams + CQRS projections | — | Decouples reads from writes, enables dedicated read replicas |
| + Verification snapshots | — | Reduces reconciliation time from hours to minutes |

## Quick Start

Enable all scalability features with a single setup:

```ts
import { Pool } from "pg";
import { drizzle } from "drizzle-orm/node-postgres";
import {
  createPooledAdapter,
  RECOMMENDED_POOL_CONFIG,
} from "@summa-ledger/drizzle-adapter";
import { createReadReplicaAdapter } from "@summa-ledger/core/db";
import { createRedisStorage } from "@summa-ledger/redis-storage";
import Redis from "ioredis";

// 1. Connection pools with production defaults
const primaryPool = new Pool({
  ...RECOMMENDED_POOL_CONFIG,
  connectionString: process.env.DATABASE_URL,
});
const replicaPool = new Pool({
  ...RECOMMENDED_POOL_CONFIG,
  connectionString: process.env.DATABASE_REPLICA_URL,
});

// 2. Read replica routing
const primary = createPooledAdapter({
  pool: primaryPool,
  drizzle: drizzle(primaryPool),
});
const replica = createPooledAdapter({
  pool: replicaPool,
  drizzle: drizzle(replicaPool),
});
const adapter = createReadReplicaAdapter({
  primary: primary.adapter,
  replicas: [replica.adapter],
});

// 3. Redis for distributed state
const redis = new Redis(process.env.REDIS_URL!);
const { storage, disconnect } = createRedisStorage({ client: redis });

// 4. Create Summa
const summa = createSumma({
  database: adapter,
  secondaryStorage: storage,
  plugins: [/* your plugins */],
});
```

## Feature Details

### Connection Pooling

Connection pooling prevents "too many connections" errors and improves throughput by reusing database connections.

```bash
pnpm add @summa-ledger/drizzle-adapter
```

`RECOMMENDED_POOL_CONFIG` provides production-ready defaults:

| Setting | Value | Purpose |
|---------|-------|---------|
| `max` | `20` | Max connections per instance |
| `min` | `5` | Warm idle connections |
| `idleTimeoutMillis` | `30000` | Close idle connections after 30s |
| `connectionTimeoutMillis` | `10000` | Fail fast if pool exhausted |
| `maxLifetimeMillis` | `1800000` | Recycle connections every 30min |
| `statement_timeout` | `30000` | Kill runaway queries after 30s |

`createPooledAdapter()` adds pool monitoring via `stats()` and graceful shutdown via `close()`.

See [Drizzle Adapter — Connection Pooling](/docs/adapters/drizzle#connection-pooling) for full reference.

### Read Replicas

Read replicas offload read queries to one or more PostgreSQL streaming replicas, freeing the primary for writes.

The simplest way is to pass a `readDatabase` adapter directly in your config:

```ts
const summa = createSumma({
  database: drizzleAdapter(primaryDb),
  readDatabase: drizzleAdapter(replicaDb),
});
```

All read-only operations (account listing, transaction lookups, search, statements, balance reads, chart-of-accounts, holds, limits) are automatically routed through `readDatabase`. Writes, transactions, and reconciliation always hit the primary.

For advanced multi-replica setups with load balancing:

```ts
import { createReadReplicaAdapter } from "@summa-ledger/core/db";

const adapter = createReadReplicaAdapter({
  primary: primaryAdapter,
  replicas: [replicaAdapter1, replicaAdapter2],
  strategy: "round-robin",  // or "random" (default)
});
```

**Routing rules:**
- Reads (`findOne`, `findMany`, `count`, SELECT queries) go to replicas
- Writes (`create`, `update`, `delete`, `rawMutate`) go to primary
- `FOR UPDATE` queries go to primary (takes row locks)
- All operations inside a `transaction()` go to primary
- Reconciliation always uses primary to avoid false positives from replication lag

<Callout type="warn" title="Replication lag">
  PostgreSQL streaming replication has a small delay (typically &lt; 100ms). Operations inside a transaction always use the primary, ensuring read-your-writes consistency where it matters.
</Callout>

### Redis Secondary Storage

Redis enables distributed state for multi-instance deployments. Without it, rate limiting is per-process only and idempotency keys aren't shared.

```bash
pnpm add @summa-ledger/redis-storage ioredis
```

```ts
import Redis from "ioredis";
import { createRedisStorage } from "@summa-ledger/redis-storage";

const { storage, disconnect, ping } = createRedisStorage({
  client: new Redis(process.env.REDIS_URL!),
  keyPrefix: "summa:",  // default
});
```

**What Redis enables:**
- **Distributed rate limiting** — `storage: "secondary"` shares counters across instances
- **Shared idempotency cache** — prevents duplicate transactions across instances
- **Plugin state caching** — read-through cache for frequently accessed data

See [Configuration — secondaryStorage](/docs/configuration#secondarystorage) for setup options.

### Hot Accounts

High-throughput accounts (e.g., a platform fee account receiving thousands of credits per second) create row-level contention — every transaction locks the same `account_balance` row with `SELECT ... FOR UPDATE`.

The `hotAccounts()` plugin solves this by **deferring balance updates** into a `hot_account_entry` table, then flushing them in batches:

```ts
import { hotAccounts } from "@summa-ledger/summa/plugins";

plugins: [
  hotAccounts({
    batchSize: 1000,      // Entries per flush (default: 1000)
    retentionHours: 24,   // Keep processed entries for 24h (default)
  }),
]
```

**How it works:**
1. During a transaction, system account entries are written to `hot_account_entry` instead of updating `system_account.balance` synchronously
2. A background worker flushes batched entries every few seconds, updating the system account balance in bulk
3. User account entries still go through the synchronous lock-and-update path (correctness guarantee)

<Callout type="warn" title="System accounts only">
  Hot account batching currently applies to **system accounts only** (e.g., `@World`, `@Fees`, `@Revenue`). User accounts always use synchronous balance updates with row locking. This is the correct design for most deployments — system accounts see N× the traffic of any single user account since they're the counterparty to every transaction.

  If a single user account genuinely receives 1000+ TPS (extremely rare), consider application-level sharding — split the logical account across multiple Summa accounts and aggregate at the application layer.
</Callout>

### Denormalized Balance Cache

Summa stores the current balance directly on the `account_balance` row via cached columns (enabled by default). This eliminates the `LATERAL JOIN` on `account_balance_version` that becomes a bottleneck at millions of version rows. To disable it (e.g., for debugging):

```ts
const summa = createSumma({
  database: adapter,
  advanced: {
    useDenormalizedBalance: false,  // default: true
  },
});
```

**How it works:**
1. Every transaction still inserts a new `account_balance_version` row (audit trail is preserved)
2. At the end of each transaction, the `account_balance` row is updated with cached balance, version, status, and checksum columns
3. All reads (`getAccount`, `listAccounts`, `getBalance`) read directly from the cached columns — no `LATERAL JOIN`
4. The `FOR UPDATE` lock and balance read happen in a single query instead of two

**Impact:** Balance reads become O(1) on the already-locked row instead of a secondary index scan. At 100M+ versions, this can reduce read latency by 10-50x.

<Callout type="info" title="Schema migration required">
  Enabling `useDenormalizedBalance` requires running `summa migrate push` to add the `cached_*` columns to the `account_balance` table. Existing data will have default values until the next transaction touches each account.
</Callout>

### Lock Retry with Backoff

High-contention accounts (many concurrent writes to the same account) can cause lock timeouts. Instead of immediately failing, configure automatic retry with exponential backoff:

```ts
advanced: {
  lockRetryCount: 3,            // Retry up to 3 times (default: 0)
  lockRetryBaseDelayMs: 50,     // Start with 50ms delay (default: 50)
  lockRetryMaxDelayMs: 500,     // Cap delay at 500ms (default: 500)
  lockMode: "nowait",           // Fail immediately if locked, then retry (default: "wait")
}
```

**Lock modes:**
- `"wait"` (default) — `FOR UPDATE` with `lock_timeout`. Blocks until the lock is available or times out.
- `"nowait"` — `FOR UPDATE NOWAIT`. Fails immediately if the row is locked, triggering a retry with backoff.
- `"optimistic"` — Skips `FOR UPDATE` entirely. Reads the current balance version without locking, then relies on the `UNIQUE(account_id, version)` constraint on `account_balance_version` to detect conflicts. On conflict (PG error `23505`), retries with jitter up to `optimisticRetryCount` times.

With `lockMode: "nowait"` and `lockRetryCount: 3`, contested accounts fail fast and retry with jitter instead of blocking in a lock queue. This significantly improves throughput on hot user accounts.

### Optimistic Locking

For accounts with very high concurrency where even `"nowait"` isn't enough, optimistic locking eliminates row-level locks entirely:

```ts
advanced: {
  lockMode: "optimistic",
  optimisticRetryCount: 3,        // Retries on version conflict (default: 3)
}
```

**How it works:**
1. Read the current balance version **without** `SELECT ... FOR UPDATE`
2. Compute the new balance, insert `entry_record` + `account_balance_version` with `version = current + 1`
3. If another transaction already inserted that version, the `UNIQUE(account_id, version)` constraint raises a `23505` error
4. Retry with jitter (10-50ms) up to `optimisticRetryCount` times

**When to use:**
- Single accounts receiving 1000+ TPS where lock queues become the bottleneck
- Low-conflict workloads where most transactions target different accounts (retry rate stays < 5%)

**When NOT to use:**
- High-conflict scenarios where the same account gets hammered (retry storms waste CPU)
- The batch engine — it uses pessimistic locking internally for deterministic ordering

On conflict, Summa throws `SummaError` with code `OPTIMISTIC_LOCK_CONFLICT` if all retries are exhausted.

### Cursor-Based Pagination

For large datasets, OFFSET-based pagination degrades as page depth increases — `OFFSET 1,000,000` requires scanning 1M rows. Summa supports cursor-based (keyset) pagination for constant-time page traversal:

```ts
// First page
const page1 = await summa.accounts.list({
  holderId: "org_123",
  limit: 20,
});
// page1.nextCursor = "eyJjcmVhdGVkX2F0..."

// Next page — O(1) regardless of depth
const page2 = await summa.accounts.list({
  holderId: "org_123",
  limit: 20,
  cursor: page1.nextCursor,
});
```

**Backward compatible:** Existing `page`/`perPage` parameters still work. When `cursor` is provided, Summa uses keyset pagination automatically. Cursor-based pagination is available on `accounts.list()`, `transactions.listAccountTransactions()`, and `holds.listAll()`.

### Version Retention

The `account_balance_version` table grows linearly with transaction volume — at 100M transactions/year, it can reach 200M+ rows. The `versionRetention()` plugin bounds this table by archiving and pruning old versions:

```ts
import { versionRetention } from "@summa-ledger/summa/plugins";

plugins: [
  versionRetention({
    retainVersions: 100,   // Keep last 100 versions per account (default)
    retainDays: 90,        // Keep versions newer than 90 days (default)
    archiveTable: true,    // Move old versions to archive table (default)
    batchSize: 500,        // Accounts per worker run (default)
    interval: "1h",        // Run every hour (default)
  }),
]
```

**How it works:**
1. A background worker identifies accounts with more versions than `retainVersions`
2. For each account, versions older than `retainDays` AND beyond the latest `retainVersions` are eligible for pruning
3. If `archiveTable: true`, eligible rows are copied to `account_balance_version_archive` before deletion
4. The archive table has the same schema plus an `archived_at` timestamp

**Impact:** At 100M txns/year with retain=100, the version table stays at ~10M rows instead of 200M+. Index maintenance and VACUUM costs are dramatically reduced.

See [Version Retention Plugin](/docs/plugins/version-retention) for full configuration.

### Table Partitioning

For very large deployments, PostgreSQL range partitioning on time-series tables dramatically improves query performance and maintenance:

```ts
import { generatePartitionDDL, partitionMaintenance } from "@summa-ledger/summa/db";

// 1. Generate migration DDL (run once)
const ddl = generatePartitionDDL({
  schema: "summa",
  tables: {
    entry_record: { type: "range", column: "created_at", interval: "monthly" },
    transaction_record: { type: "range", column: "created_at", interval: "monthly" },
    ledger_event: { type: "range", column: "created_at", interval: "monthly" },
  },
});
// Returns SQL statements to run in a maintenance window

// 2. Enable automatic partition maintenance
plugins: [
  partitionMaintenance({
    tables: {
      entry_record: { interval: "monthly" },
      transaction_record: { interval: "monthly" },
      ledger_event: { interval: "monthly" },
    },
    createAhead: 3,            // Create partitions 3 months ahead
    retainPartitions: 24,      // Detach partitions older than 24 months
    requireSealedBlocks: true, // Block detachment if events aren't checkpointed (default)
    archiveSchema: "summa_archive", // Move detached partitions here instead of dropping
  }),
]
```

**Partition intervals:** `"daily"`, `"weekly"`, or `"monthly"`.

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `createAhead` | `number` | `3` | Create partitions this many intervals ahead |
| `retainPartitions` | `number \| null` | `null` | Detach partitions older than this many intervals. `null` = never detach |
| `requireSealedBlocks` | `boolean` | `true` | Block partition detachment unless all events are covered by sealed block checkpoints. Prevents hash chain breakage |
| `archiveSchema` | `string` | — | Move detached partitions to this schema (created automatically) instead of leaving as standalone tables |
| `workerInterval` | `string` | `"1d"` | How often the maintenance worker runs |

**Hash chain safety:** When `requireSealedBlocks` is enabled (default), the maintenance worker calls `canSafelyDetachPartition()` before detaching `ledger_event`, `entry_record`, or `account_balance_version` partitions. If any events in the partition's date range aren't covered by a sealed block checkpoint, detachment is blocked and a warning is logged. Run `createBlockCheckpoint()` first.

```ts
import { canSafelyDetachPartition } from "@summa-ledger/summa/db";

// Check if a partition can be safely detached
const result = await canSafelyDetachPartition(ctx, "2025-01-01", "2025-02-01");
if (!result.safe) {
  console.log(result.reason);        // Why it's blocked
  console.log(result.unsealedCount); // Events not yet checkpointed
}
```

**Impact:** Queries with `created_at` filters hit only relevant partitions. VACUUM and index maintenance happens per-partition. Detaching old partitions is instant (vs DELETE which causes table bloat).

<Callout type="warn" title="One-time migration required">
  Converting existing tables to partitioned tables requires a maintenance window. The `generatePartitionDDL()` helper produces the exact SQL statements, but you must run them manually. Back up your database first.
</Callout>

### Transaction Batching (Batch Engine)

For the highest throughput, enable the `batchEngine()` plugin. Inspired by [TigerBeetle's](https://tigerbeetle.com/) batching architecture, it buffers incoming transactions and processes them in a single database transaction using multi-row `UNNEST` INSERTs — amortizing per-transaction overhead across the entire batch.

```ts
import { batchEngine } from "@summa-ledger/summa/plugins";

const summa = createSumma({
  database: adapter,
  plugins: [
    batchEngine({
      maxBatchSize: 200,        // Transactions per batch (default: 200)
      flushIntervalMs: 5,       // Max wait before flushing (default: 5ms)
    }),
  ],
  advanced: {
    enableBatching: true,       // Required to activate
  },
});
```

**How it works:**
1. Each `creditAccount()` / `debitAccount()` call is buffered in-memory instead of executing immediately
2. When the buffer reaches `maxBatchSize` or `flushIntervalMs` elapses, the batch is flushed
3. All accounts in the batch are locked in sorted order (prevents deadlocks)
4. All balances, checksums, and HMAC hashes are computed in-memory
5. All writes (transaction_record, entry_record, account_balance_version, ledger_event, outbox, velocity log) are inserted via multi-row `UNNEST` — one query per table, not one per transaction
6. Each transaction in the batch has its own `resolve`/`reject` — validation failures reject individual transactions without failing the batch

**Impact:** At batch size 200 with 20 DB connections, each batch completes in ~10-20ms. That's 200 transactions per 15ms = **10,000-13,000 TPS per connection**, or **60,000-120,000+ TPS** with a 20-connection pool.

<Callout type="info" title="Security preserved">
  Batching does NOT compromise any security guarantees. Every transaction still gets its own HMAC hash chain entry, balance checksum, immutable event record, and outbox webhook. The only difference is *when* the writes hit the database — batched together instead of one-at-a-time.
</Callout>

<Callout type="warn" title="Latency trade-off">
  Batching adds up to `flushIntervalMs` (default: 5ms) of latency to each transaction. For latency-sensitive workloads, keep `flushIntervalMs` low or disable batching and rely on the mega CTE optimization instead (~3,000 TPS with sub-millisecond overhead).
</Callout>

### Mega CTE (Automatic)

Even without batching enabled, Summa automatically combines all per-transaction writes into a single PostgreSQL CTE (Common Table Expression). Instead of 6-7 sequential INSERT statements, every `creditAccount()` / `debitAccount()` executes a single SQL statement that inserts into all tables at once:

```sql
WITH new_txn AS (INSERT INTO transaction_record ...),
     new_status AS (INSERT INTO transaction_status ...),
     new_entry AS (INSERT INTO entry_record ...),
     new_version AS (INSERT INTO account_balance_version ...),
     new_event AS (INSERT INTO ledger_event ...),
     new_outbox AS (INSERT INTO outbox ...)
SELECT id, created_at FROM new_txn;
```

This optimization is always active and requires no configuration. It reduces per-transaction round-trips from ~10 to ~3, roughly tripling throughput compared to the sequential approach.

### Streaming Statement Generation

For large statements (10K+ entries), the in-memory generators (`generateStatementPdf`, `generateStatementCsv`) buffer everything before returning. Use the streaming variants to pipe directly to an HTTP response with bounded memory:

```ts
import { generateStatementPdfStream, generateStatementCsvStream } from "@summa-ledger/summa/plugins";

// PDF — returns a Readable stream (PDFDocument)
const pdfStream = await generateStatementPdfStream(ctx, "user_123", {
  dateFrom: "2026-01-01",
  dateTo: "2026-02-01",
});
pdfStream.pipe(res); // Pipe directly to HTTP response

// CSV — returns an AsyncGenerator<string>
const csvStream = generateStatementCsvStream(ctx, "user_123", {
  dateFrom: "2026-01-01",
});
for await (const row of csvStream) {
  res.write(row);
}
res.end();
```

Both stream entries in 500-row batches internally — memory usage stays constant regardless of statement size. See [Statements Plugin](/docs/plugins/statements#streaming-generation) for full API.

### Background Statement Generation

Large CSV/PDF statement generation can cause request timeouts. The statements plugin includes a background worker that processes generation jobs asynchronously.

```ts
// Submit a job
const res = await fetch("/statements/user_123/generate", {
  method: "POST",
  body: JSON.stringify({ format: "pdf", dateFrom: "2026-01-01" }),
});
const { jobId } = await res.json();

// Poll for result
const job = await fetch(`/statements/jobs/${jobId}`);
// { status: "completed", content: "base64...", filename: "..." }
```

The `statement-generator` worker runs every 5 seconds and processes up to 5 pending jobs per cycle. See [Statements Plugin](/docs/plugins/statements#async-background-generation) for full API reference.

## Graceful Shutdown

When scaling across multiple instances, always shut down cleanly:

```ts
async function shutdown() {
  // 1. Stop accepting new requests
  server.close();

  // 2. Stop background workers (releases distributed leases)
  await summa.workers.stop();

  // 3. Disconnect Redis
  await disconnect();

  // 4. Drain connection pools
  await primary.close();
  await replica.close();

  process.exit(0);
}

process.on("SIGTERM", shutdown);
process.on("SIGINT", shutdown);
```

## Monitoring

### Pool Health

Monitor connection pool exhaustion — the #1 cause of latency spikes:

```ts
setInterval(() => {
  const s = primary.stats();
  if (s.waitingCount > 0) {
    logger.warn("Pool exhaustion", {
      active: s.activeCount,
      waiting: s.waitingCount,
      total: s.totalCount,
    });
  }
}, 30_000);
```

### Redis Health

```ts
const healthy = await ping();
if (!healthy) {
  logger.error("Redis unreachable — rate limiting degraded");
}
```

## Tier 2: Beyond 40K TPS

The features above (Tier 1) get you to ~40K-60K TPS. Tier 2 removes secondary bottlenecks — outbox polling throughput, event table growth, read/write contention, and reconciliation time — to push beyond 100K TPS with **zero new infrastructure** (still just PostgreSQL + Redis + Node.js).

### Event Store Partitioning

As `ledger_event` grows past 100M rows, sequential scans and index maintenance slow down. PostgreSQL range partitioning splits the table by `created_at` into monthly (or weekly) partitions — queries automatically prune irrelevant partitions.

```ts
import { createSumma } from "@summa-ledger/summa";
import { eventStorePartition } from "@summa-ledger/summa/plugins";

const summa = createSumma({
  database: adapter,
  currency: "USD",
  plugins: [
    eventStorePartition({
      interval: "monthly",   // or "weekly" for very high volume
      createAhead: 3,        // pre-create 3 future partitions
      retainPartitions: 24,  // archive partitions older than 24 months
    }),
  ],
});
```

Generate the initial migration SQL (does NOT auto-execute):

```bash
npx summa partition generate --interval monthly
# Writes: migrations/partition_ledger_event_<timestamp>.sql
```

Check partition status:

```bash
npx summa partition status
# Shows: partitioned (yes/no), child partitions, row counts, sizes
```

<Callout type="warn" title="Run during a maintenance window">
  The generated migration renames the existing table, creates the partitioned replacement, and migrates data. Review the SQL carefully and execute during low-traffic periods.
</Callout>

### Redis Streams Message Queue

The outbox polling pattern tops out at ~20 msg/s. Redis Streams pushes this to 50K-100K msg/s with sub-millisecond latency — using your existing Redis instance.

```bash
pnpm add @summa-ledger/message-queue
```

```ts
import { createSumma } from "@summa-ledger/summa";
import { outbox } from "@summa-ledger/summa/plugins";
import { createRedisStreamsBus } from "@summa-ledger/message-queue";
import Redis from "ioredis";

const redis = new Redis(process.env.REDIS_URL);
const bus = createRedisStreamsBus({ client: redis });

const summa = createSumma({
  database: adapter,
  currency: "USD",
  plugins: [
    outbox({ messageQueue: bus }),
  ],
});
```

This is a drop-in upgrade — just pass your Redis Streams bus to the `messageQueue` option. All downstream consumers (webhooks, projections, external systems) receive events via Redis Streams consumer groups instead of polling.

See [Message Queue Plugin](/docs/plugins/message-queue) for standalone usage and consumer group configuration.

### CQRS Projections

Separate read and write paths. Writes go to the primary database, reads are served from denormalized projection tables on a read replica — eliminating read/write contention entirely.

```bash
pnpm add @summa-ledger/projections @summa-ledger/message-queue
```

```ts
import { createSumma } from "@summa-ledger/summa";
import { outbox } from "@summa-ledger/summa/plugins";
import { createRedisStreamsBus } from "@summa-ledger/message-queue";
import {
  projectionRunner,
  AccountBalanceProjection,
  TransactionHistoryProjection,
} from "@summa-ledger/projections";
import Redis from "ioredis";

const redis = new Redis(process.env.REDIS_URL);
const bus = createRedisStreamsBus({ client: redis });

const summa = createSumma({
  database: adapter,
  currency: "USD",
  plugins: [
    outbox({ messageQueue: bus }),
    projectionRunner(bus, [
      AccountBalanceProjection,
      TransactionHistoryProjection,
    ]),
  ],
});
```

Projections are **eventually consistent** (typically under 5 seconds). For strict consistency, query the primary database directly. For full read/write separation with a dedicated replica:

```ts
import { createCQRSAdapter } from "@summa-ledger/projections";

const cqrs = createCQRSAdapter({
  readAdapter: replicaAdapter,   // Connected to read replica
  writeAdapter: primaryAdapter,  // Connected to primary
});
```

See [Projections Plugin](/docs/plugins/projections) for custom projections and architecture details.

### Verification Snapshots

Daily reconciliation verifies the entire event hash chain — O(all events). With millions of events, this takes hours. Verification snapshots store the last verified hash per aggregate, reducing verification to O(events since last snapshot).

```ts
import { createSumma } from "@summa-ledger/summa";
import { verificationSnapshots } from "@summa-ledger/summa/plugins";

const summa = createSumma({
  database: adapter,
  currency: "USD",
  plugins: [
    verificationSnapshots({
      snapshotInterval: "6h",  // Create snapshots every 6 hours
      batchSize: 500,          // Process 500 aggregates per worker cycle
    }),
  ],
});
```

| Aggregates | Events | Without snapshots | With snapshots |
|:---:|:---:|:---:|:---:|
| 10K | 1M | ~15 min | ~2 min |
| 100K | 50M | ~14 hours | ~45 min |
| 1M | 500M | days | ~3 hours |

The `hash-snapshot-creator` worker runs automatically, creating snapshots for aggregates that have accumulated new events since the last snapshot. The reconciliation plugin automatically uses snapshots when available — no configuration changes needed.

See [Verification Snapshots Plugin](/docs/plugins/hash-snapshot) for schema details and verification API.

### Full Tier 2 Setup

Combining all Tier 2 features:

```ts
import { createSumma, createDrizzleAdapter } from "@summa-ledger/summa";
import {
  outbox, eventStorePartition, verificationSnapshots,
} from "@summa-ledger/summa/plugins";
import { createRedisStreamsBus } from "@summa-ledger/message-queue";
import {
  projectionRunner,
  AccountBalanceProjection,
  TransactionHistoryProjection,
} from "@summa-ledger/projections";
import Redis from "ioredis";

const redis = new Redis(process.env.REDIS_URL);
const bus = createRedisStreamsBus({ client: redis });

const summa = createSumma({
  database: adapter,
  currency: "USD",
  plugins: [
    // Tier 2: Event Store Partitioning
    eventStorePartition({ interval: "monthly", createAhead: 3 }),

    // Tier 2: Redis Streams (replaces outbox polling)
    outbox({ messageQueue: bus }),

    // Tier 2: CQRS Projections
    projectionRunner(bus, [
      AccountBalanceProjection,
      TransactionHistoryProjection,
    ]),

    // Tier 2: Verification Snapshots (faster reconciliation)
    verificationSnapshots({ snapshotInterval: "6h" }),
  ],
});
```

## What's Next

With Tier 1 + Tier 2 optimizations, Summa can exceed 100,000 TPS on a single PostgreSQL instance with constant-time query performance regardless of data volume. For workloads beyond that, consider:
- **Database sharding** — shard by account for horizontal write scaling
- **Multiple Summa instances** — horizontal scaling with Redis-backed distributed state
